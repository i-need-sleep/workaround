<template>

  <!-- Title section -->
  <h1 class="title">Aligning Text-to-Music Evaluation with Human Preferences</h1>

   <!-- TODO: Authors and link -->
   <!-- <p>[Anonymos authors]</p> -->
   <br>

   <hr>

   <br>

   <p class="tldr">
    We propose <b><span class="bold_italic">MAD</span></b>: an automatic evaluation metric for music generation motivated by <b> holistic meta-evaluation with synthetic data </b>
    and validated on <b> <span class="bold_italic">MusicPrefs</span>, the first open-source dataset of human preferences for text-to-music generation</b>.
   </p>

   <br>

   <img src="./assets/figure_1.png" alt="Figure 1" width="100%">

   <br>

  <!-- Abstract -->
  <h1>Abstract</h1>
  <hr>

  <p>
    Despite significant recent advances in generative acoustic text-to-music (TTM) modeling, 
    robust evaluation of these models lags behind, relying in particular on the popular Fr√©chet Audio Distance~(FAD). 
    In this work, we rigorously study the design space of reference-based divergence metrics for evaluating TTM models 
    through (1) designing four synthetic meta-evaluations to measure sensitivity to particular musical desiderata, and 
    (2) collecting <span class="bold_italic">MusicPrefs</span>, the first open-source dataset of human preferences for TTM systems. 
    We find that not only is the standard FAD setup inconsistent on both synthetic and human preference data, 
    but that nearly <i>all</i> existing metrics fail to effectively capture desiderata, and are only weakly correlated with human perception. 
    We propose a new metric, the <span class="bold_italic">MAD</span>, computed on representations from a self-supervised audio embedding model. 
    We find that this metric effectively captures diverse musical desiderata (avg. rank correlation 0.84 for MAD vs. 0.49 for FAD) 
    and also correlates more strongly with MusicPrefs (0.62 vs. 0.14).
  </p>

  <br>

  <!-- Synthetic meta-eval demo -->
  <h1>Meta-Evaluation with Synthesised Data</h1>
  <hr>

  <p>
    To explore the design space of reference-based music metrics, we construct a set of four meta-evaluations designed to specifically 
    disentangle different desiderata in text-to-music systems, each considering a distinct axis: 
    <i>audio fidelity</i>, <i>musicality</i>, <i>context</i>, and <i>diversity</i>. Within each meta-evaluation, 
    we introduce increasing levels of distortion. In this way, we are able to capture whether a given metric actually captures
    specific forms of musical degradation, and allows us to measure the sensitivity of each metric to changes in distortion strength as well as embedding backbone.
    Details are discussed in Section 3 of the paper.
  </p>

  <p>
    Examples: <br>
    <button @click="synth_click_left"><</button>
    Set {{ synth_idx + 1 }} of {{ synth_max_idx }}
    <button @click="synth_click_right">></button>
  </p>

  <div class="grid-container_4">

    <div class="grid-container-sub" v-for="distortion_type in ['Fidelity', 'Musicality', 'Context', 'Diversity']" :key="distortion_type">
      <b class="audio_col_label">{{ distortion_type }}</b>

      <div v-for="distortion_idx in Array(synth_n_levels).keys()" :key="distortion_idx">

        <audio controls :key="this.get_synth_url(distortion_type, distortion_idx)" preload="none">
          <source :key="this.get_synth_url(distortion_type, distortion_idx)" :src="this.get_synth_url(distortion_type, distortion_idx)" type="audio/wav">
          Your browser does not support the audio element.
        </audio>
        <p class="audio_caption">{{ this.get_synth_caption(distortion_type, distortion_idx) }}</p>

      </div>

    </div>

  </div>

  <!-- MusicPrefs: Human data -->
  <h1>MusicPrefs: Human Preferences for Text-to-Music Generation</h1>
  <hr>

  <p>
    To validate MAD, we collect and release a large set of human preferences on music generated by state-of-the-art open-weights text-to-music systems.
    We consider Stable Audio Open, MusicGen (small/medium/large), AudioLDM2, MusicLDM, and Riffusion v1. 
    Annotators are presented with audio generated by two different systems and asked which of the pair they prefer (ties are allowed) in terms of fidelity and musicality.
    More details and results are discussed in Sections 4 and 5 of the paper. 
  </p>

  <p>
    Examples: <br>
    <button @click="musicprefs_click_left"><</button>
    Set {{ musicprefs_idx + 1 }} of {{ musicprefs_max_idx }}
    <button @click="musicprefs_click_right">></button><br>
    Hide annotations <input type="checkbox" v-model="musicprefs_hide_annotation">
  </p>

  
  <div class="grid-container_2">
    <div>
      <audio controls :key="get_musicprefs_url_a()" preload="none" class="music-prefs-audio">
        <source :key="get_musicprefs_url_a()" :src="get_musicprefs_url_a()" type="audio/wav">
        Your browser does not support the audio element.
      </audio>
      <p>System A<span v-if="!musicprefs_hide_annotation">: <br>{{ get_musicprefs_system_a() }}</span></p>
    </div>

    <div>
      <audio controls :key="get_musicprefs_url_b()" preload="none" class="music-prefs-audio">
        <source :key="get_musicprefs_url_b()" :src="get_musicprefs_url_b()" type="audio/wav">
        Your browser does not support the audio element.
      </audio>
      <p>System B<span v-if="!musicprefs_hide_annotation">: <br>{{ get_musicprefs_system_b() }}</span></p>
    </div>

  </div>

  <p v-if="!musicprefs_hide_annotation">
    Human preference: <br>
    <b>Fidelity:</b> {{ get_musicprefs_fidelity() }} <br>
    <b>Musicality:</b> {{ get_musicprefs_musicality() }}
  </p>

  <!-- MusicPrefs: More generated data -->
  <h1>MusicPrefs: More Generated Music</h1>
  <hr>

  <p>
    Here we show more unannotated examples of music generated by the systems in MusicPrefs alongside the cleaned prompts used to generate them.
  </p>

  <!-- Mode selection -->
  <div>
    View mode:
    <select v-model="mode">
      <option>Same prompt, different systems</option>
      <option>Same prompt, same system, different seeds</option>
    </select>
  </div>

  <!-- Prompt id selection -->
  <div>
    Prompt id (0 - {{ metadata.length - 1 }}):
    <input v-model="prompt_id"/>
    <button @click="randomize_prmompt_id">Random</button>
  </div>

  <br>

  <!-- Prompt -->
  <div>
    <p>Prompt:</p> 
    {{ metadata[prompt_id].prompt }}
  </div>

  <br>

  <!-- Mode-specific stuff -->
  <div v-if="mode === 'Same prompt, different systems'"  class="grid-container_4">
    <div v-for="system in ['musicgen_large', 'musicgen_medium', 'musicgen_small', 'musicldm', 'stable_audio_open', 'audioldm2', 'riffusion_v1']" :key="system">
      <div v-if="(system !== 'prompt') && (system !== 'musiccaps_prompt_id') && (system !== 'riffusion_inst')">
        <audio controls :key="prompt_id" preload="none">
          <source :key="prompt_id" :src="'/'+metadata[prompt_id][system][0]" type="audio/wav">
          Your browser does not support the audio element.
        </audio>
        <p>{{ model_names_reformat[system] }}</p>
      </div>
    </div>
  </div>

  <div v-else-if="mode === 'Same prompt, same system, different seeds'">
    System: 
    <select v-model="system">
      <option v-for="system in get_systems()" :key="system">
        {{ model_names_reformat[system] }}
      </option>
    </select>
    <div class="grid-container_4">
      <div v-for="url in more_data_get_urls()" :key="url">
        <audio controls :key="url" preload="none">
          <source :key="url" :src="'/'+url" type="audio/wav">
          Your browser does not support the audio element.
        </audio>
      </div>
    </div>
  </div>

</template>


<script>

// Read the metadata jsons
import synth_metadata from './assets/synth_samples/metadata.json'
import musicprefs_metadata from './assets/musicprefs_metadata.json'
import metadata from './assets/demo_metadata.json'

export default {
  data() {
    return {
      // self_url: "http://127.0.0.1:5000",
      self_url: "https://humanratings-6207bc1e725a.herokuapp.com/",

      // Synthetic data
      synth_metadata: synth_metadata,
      synth_max_idx: 21,
      synth_n_levels: 11,

      synth_idx: 0,

      // MusicPrefs
      musicprefs_metadata: musicprefs_metadata,
      musicprefs_max_idx: 21,
      model_names_reformat:{
        "musicgen_large": "MusicGen-large",
        "musicgen_medium": "MusicGen-medium",
        "musicgen_small": "MusicGen-small",
        "riffusion_v1": "Riffusion v1",
        "stable_audio_open": "Stable Audio Open",
        "audioldm2": "AudioLDM2",
        "musicldm": "MusicLDM"
      },
      model_names_reverse_format: {
        "MusicGen-large": "musicgen_large",
        "MusicGen-medium": "musicgen_medium",
        "MusicGen-small": "musicgen_small",
        "Riffusion v1": "riffusion_v1",
        "Stable Audio Open": "stable_audio_open",
        "AudioLDM2": "audioldm2",
        "MusicLDM": "musicldm"
      },

      musicprefs_idx: 0,
      musicprefs_hide_annotation: false,

      // Addutinal music
      // Constants
      metadata: metadata,

      // Variables
      mode: 'Same prompt, different systems',
      prompt_id: '0',
      system: 'MusicGen-large'

    };
  },
  methods: {
    get_synth_url(distortion_type, distortion_level) {
      return this.synth_metadata[distortion_type.toLowerCase()][distortion_level * this.synth_max_idx + this.synth_idx][0].replace('./synth_samples', this.self_url+'sdb_audio/generated')
    },

    get_synth_caption(distortion_type, distortion_level) {
      let distortion_val = this.synth_metadata[distortion_type.toLowerCase()][distortion_level * this.synth_max_idx + this.synth_idx][1]
      if (distortion_type == 'Fidelity'){
        return 'White noise std.: ' + distortion_val
      }
      if (distortion_type == 'Musicality'){
        return 'Note perturbation prob.: ' + distortion_val
      }
      if (distortion_type == 'Context'){
        return 'Max. context len. (secs.): ' + distortion_val / 50
      }
      if (distortion_type == 'Diversity'){
        return 'Num. unique prompts: ' + distortion_val
      }
    },

    synth_click_left() {
      this.synth_idx = (this.synth_idx - 1 + this.synth_max_idx) % this.synth_max_idx
    },
    synth_click_right() {
      this.synth_idx = (this.synth_idx + 1) % this.synth_max_idx
    },

    musicprefs_click_left() {
      this.musicprefs_idx = (this.musicprefs_idx - 1 + this.musicprefs_max_idx) % this.musicprefs_max_idx
    },
    musicprefs_click_right() {
      this.musicprefs_idx = (this.musicprefs_idx + 1) % this.musicprefs_max_idx
    },

    get_musicprefs_url_a() {
      return musicprefs_metadata[this.musicprefs_idx + 3]["audio_a_url"]
    },
    get_musicprefs_url_b() {
      return musicprefs_metadata[this.musicprefs_idx + 3]["audio_b_url"]
    },
    get_musicprefs_system_a() {
      return this.model_names_reformat[musicprefs_metadata[this.musicprefs_idx + 3]["system_a"]]
    },
    get_musicprefs_system_b() {
      return this.model_names_reformat[musicprefs_metadata[this.musicprefs_idx + 3]["system_b"]]
    },
    get_musicprefs_fidelity() {
      return ["System A", "Tie", "System B"][musicprefs_metadata[this.musicprefs_idx + 3]["fidelity"]]
    },
    get_musicprefs_musicality() {
      return ["System A", "Tie", "System B"][musicprefs_metadata[this.musicprefs_idx + 3]["musicality"]]
    },


    randomize_prmompt_id() {
      this.prompt_id = Math.floor(Math.random() * this.metadata.length).toString()
    },
    get_systems() {
      return ['musicgen_large', 'musicgen_medium', 'musicgen_small', 'musicldm', 'stable_audio_open', 'audioldm2', 'riffusion_v1']
    },
    more_data_get_urls() {
      console.log(this.metadata[this.prompt_id][this.model_names_reverse_format[this.system]] )
      return this.metadata[this.prompt_id][this.model_names_reverse_format[this.system]] 
    }
  }

};
</script>

<style>

.music-prefs-audio {
  margin-left: 2%;
  width: 500px;
}

body {
  color: black;
  font-size: large;
  margin-left: 10%;
  margin-right: 10%;
  margin-top: 1%;
  margin-bottom: 5%;
}

p {
  padding: 2%;
}

h1 {
  font-size: xx-large;
}

b.tldr {
  font-weight: bold;
  color: #01796F;
}

b {
  font-weight: bold;
}

audio {
  width: 200px;
}

.title {
  font-size: xx-large;
  font-weight: bold;
}

.tldr {
  /* font-size: larger; */
  background-color: #c9c9c9;
  border-radius: 5px;
}

.bold_italic {
  font-weight: bold;
  font-style: italic;
}

.grid-container_4 {
  display: grid;
  grid-template-columns: auto auto auto auto;
  grid-gap: 10px;
  padding: 10px;
}

.grid-container_2 {
  display: grid;
  grid-template-columns: auto auto;
  grid-gap: 10px;
  padding: 10px;
}

.grid-container-sub {
  display: grid;
  grid-template-columns: auto;
  grid-gap: 10px;
  padding: 10px;
}

.audio_caption {
  /* text-align: center; */
  /* No margin or padding */
  margin: 0;
  padding: 0;
  white-space: nowrap;
  font-size: small;
}

.audio_col_label {
  font-weight: bold;
  /* text-align: center; */
}

</style>
